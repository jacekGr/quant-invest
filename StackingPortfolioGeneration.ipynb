{
  "cells": [
    {
      "metadata": {
        "_uuid": "bd5f1d9bc2787173627c08a45335e8c556d5d61f"
      },
      "cell_type": "markdown",
      "source": "# Stacking\n\n### Idea\nBecause training base models is hard, the possible optimal solution may be to take a lot of classical models\nfor portfolio creation and train a 2nd-layer model on their outputs, to select an optimal output.\n\nThe challenges we need to solve in this notebook are the following:\n- selecting a target variable and metric\n- generating as many simple models as possible\n- adding features related to the long-term trends for 2nd layer model for it to perform a \"more informed\" choice\n- choosing whether base models should be allowed to see entire past that we have or just the most recent fraction of it"
    },
    {
      "metadata": {
        "_uuid": "2a4221e9ddce1a7863734898271753cbe709488a"
      },
      "cell_type": "markdown",
      "source": "### Loading data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ecafbaf7c5843221d9c4a53209c77c2139dcb4a4"
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\nprefix = '../input/'\nFUNDS_CSV = prefix+'Quant_Invest_Fundusze.csv'\nSTOCKS_CSV = prefix+'all_indices_close.csv'\nCOMMODITIES_CSV = prefix+'all_commodities_close.csv'\nRATES_CSV = prefix+'policy_rates.csv'\nFX_CSV = prefix+'exchange_rates.csv'\n\n\ndef _load_indexed(path, index_col='Daty', sep=',') -> pd.DataFrame:\n    df = pd.read_csv(path, index_col=index_col, sep=sep)\n    df.index = pd.to_datetime(df.index)\n    return df.sort_index()\n\n\ndef load_funds() -> pd.DataFrame:\n    return _load_indexed(FUNDS_CSV, sep=';')\n\n\ndef load_stocks() -> pd.DataFrame:\n    return _load_indexed(STOCKS_CSV, index_col='Data')\n\n\ndef load_commodities() -> pd.DataFrame:\n    return _load_indexed(COMMODITIES_CSV, index_col='Data')\n\n\ndef load_rates() -> pd.DataFrame:\n    policy_rates = pd.read_csv(RATES_CSV, index_col = 0)\n    policy_rates = policy_rates.pivot(index='date', columns='reference_area', values='obs_value')\n    policy_rates.index = pd.to_datetime(policy_rates.index)\n    return policy_rates.iloc[:(-42)]  # keep only years 2000-2018\n\n\ndef load_fx() -> pd.DataFrame:\n    exchange_rates = pd.read_csv(FX_CSV, index_col=0)\n    exchange_rates.index = pd.to_datetime(exchange_rates['Date'])\n    shortnames={}\n    for col in exchange_rates.columns[1:]:\n        start = col.index('(')\n        end = col.index(')')\n        shortnames[col] = col[(start+1):end]\n    return exchange_rates.rename(columns = shortnames).drop(columns=['Date'])\n\n\ndef load_all() -> pd.DataFrame:\n    return pd.concat([\n        load_funds(),\n        load_stocks(),\n        load_commodities(),\n        load_rates(),\n        load_fx()\n    ], axis='columns')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a763f58fb9706dbe862e20d0b1f03fd346b39151"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0af4920ac03e62c2224bc0b28fbcb707b9d714ec"
      },
      "cell_type": "code",
      "source": "df = load_all()\ndf = df.loc[~df['AP'].isna()]\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73facd5adea37f025ec20c6b03eb52f46cb35f47"
      },
      "cell_type": "code",
      "source": "df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de6c997ceaabcb997e0b8e13ec31fc87b9475643"
      },
      "cell_type": "code",
      "source": "fund_colnames = ['AP', 'ARR', 'ARW', 'G', 'OP', 'ORR', 'ORW']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7a1f9c7177977c908e17fb5dde30319bab496f6"
      },
      "cell_type": "code",
      "source": "funds_df = df[fund_colnames]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1226642bf38319d87fa6d38fce298c04bd8d0d41"
      },
      "cell_type": "code",
      "source": "funds_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4a4ff4891c07180f73b17c339fd5d797647a88fe"
      },
      "cell_type": "markdown",
      "source": "### Feature and target selection\n\nFirstly, we will try to train a classifier that selects the best performing model."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31667a880b94deca90db5308fe3d5c749e3e3fff"
      },
      "cell_type": "code",
      "source": "from typing import List, Callable, Tuple\nfrom dataclasses import dataclass, asdict\n\nfrom pypfopt.expected_returns import mean_historical_return\nfrom pypfopt.risk_models import CovarianceShrinkage\nfrom pypfopt.efficient_frontier import EfficientFrontier\n\nfrom numba import jit\n\nfrom tqdm import tqdm",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d47f3fa792ba1d7e3ac3b7a976c0edfc9067b71"
      },
      "cell_type": "code",
      "source": "year_days = int(funds_df.groupby(funds_df.index.year).count().mean().mean())\nmonth_days = int(year_days/12)\nweek_days = int(month_days/4)\nprint(f\"Using default number of trading days in a year={year_days}, month={month_days}, week={week_days}\")\n\nmax_volatility = 0.1\nprint(f\"Using volatility threshold={max_volatility}\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08878de38e0a9f151e715f17cc5cb6a579b4466b"
      },
      "cell_type": "code",
      "source": "@dataclass\nclass Portfolio(object):\n    AP: float\n    ARR: float\n    ARW: float\n    G: float\n    OP: float\n    ORR: float\n    ORW: float\n\n    def as_weights(self):\n        return np.array(list(asdict(self).values()))\n\n\ndef risk_free_rate(table, interval_len_in_days: int=1) -> int:\n    \"\"\" For the purpose of this portfolio selection, we use G (cash fund returns) as a risk-free rate. \"\"\"\n    intervals_in_a_year = year_days / interval_len_in_days\n    return np.mean((df['G'].shift(1) - df['G'].iloc[:-1]) / df['G'].iloc[:-1]) * intervals_in_a_year\n\n\ndef _ef_builder(table):\n    \"\"\" Builder for all kinds of efficient frontier models. \"\"\"\n    table = table.copy().groupby(by=[table.index.year, table.index.month]).tail(n=1)\n    mu = mean_historical_return(table)\n    S = CovarianceShrinkage(table).ledoit_wolf()\n    return EfficientFrontier(mu, S)\n\ndef _ef_meta_builder(ef):\n    \"\"\" \n    Common metadata for all kinds of efficient frontier models:\n    expected annualized returns, expected volatility, sharpe ratio\n    \"\"\"\n    return np.array(ef.portfolio_performance())\n\ndef ef_max_sharpe(table, interval: int=1):\n    ef = _ef_builder(table)\n    weights = ef.max_sharpe(risk_free_rate(interval))\n    return Portfolio(**weights), _ef_meta_builder(ef)\n\ndef ef_min_volatility(table, interval: int=1):\n    ef = _ef_builder(table)\n    weights = ef.min_volatility()\n    return Portfolio(**weights), _ef_meta_builder(ef)\n\ndef ef_efficient_risk(table, risk_target: float, interval: int=1):\n    ef = _ef_builder(table)\n    weights = ef.efficient_risk(risk_target, risk_free_rate=risk_free_rate(interval))\n    return Portfolio(**weights), _ef_meta_builder(ef)\n\ndef ef_efficient_return(table, target_return, interval: int=1):\n    ef = _ef_builder(table)\n    weights = ef.efficient_return(target_return)\n    return Portfolio(**weights), _ef_meta_builder(ef)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b05ec7accde6088e6b631cddbec0339e8b8ed7ee"
      },
      "cell_type": "code",
      "source": "def calculate_features(funds_df: pd.DataFrame) -> Tuple[np.array, List[Portfolio]]:\n    \"\"\"\n    Returns features for the model along with the portfolios that were used for calculating these features.\n    \"\"\"\n    monthly_table = funds_df.groupby([funds_df.index.year, funds_df.index.month]).tail(1)\n    weekly_table = funds_df.groupby([funds_df.index.year, funds_df.index.week]).tail(1)\n    portfolio_data = [\n        ef_max_sharpe(funds_df),\n        ef_max_sharpe(monthly_table, month_days),\n        ef_max_sharpe(weekly_table, week_days),\n        ef_min_volatility(funds_df),\n        ef_min_volatility(monthly_table, month_days),\n        ef_min_volatility(weekly_table, week_days),\n        ef_efficient_risk(funds_df, 0.10),\n        ef_efficient_risk(funds_df, 0.05),\n        ef_efficient_risk(monthly_table, 0.10, month_days),\n        ef_efficient_risk(monthly_table, 0.05, month_days),\n        ef_efficient_risk(weekly_table, 0.10, week_days),\n        ef_efficient_risk(weekly_table, 0.05, week_days),\n        ef_efficient_return(funds_df, 0.04),\n        ef_efficient_return(funds_df, 0.08),\n        ef_efficient_return(monthly_table, 0.04, month_days),\n        ef_efficient_return(monthly_table, 0.08, month_days),\n        ef_efficient_return(weekly_table, 0.04, week_days),\n        ef_efficient_return(weekly_table, 0.08, week_days),\n    ]\n    portfolios = [pd[0] for pd in portfolio_data]\n    portfolio_weights = np.hstack([p.as_weights() for p in portfolios])\n    portfolio_features = np.hstack([pd[1] for pd in portfolio_data])\n    return np.hstack([portfolio_weights, portfolio_features]), portfolios\n    \n\n@jit(nopython=True)\ndef portfolio_performance(portfolio_allocation: np.array, fund_df_values: np.array) -> Tuple[float, float]:\n    fund_returns = (fund_df_values[-1] - fund_df_values[0]) / fund_df_values[0]\n    portfolio_returns = np.sum(portfolio_allocation*fund_returns) / np.sum(portfolio_allocation)\n    # eliminating portfolios based on too high volatility\n    portfolio_period_values = np.sum(portfolio_allocation*fund_df_values[:-1], axis=1) / np.sum(portfolio_allocation)\n    portfolio_period_shifted = np.sum(portfolio_allocation*fund_df_values[1:], axis=1) / np.sum(portfolio_allocation)\n    portfolio_daily_returns = (portfolio_period_shifted - portfolio_period_values) / portfolio_period_values\n    portfolio_volatility = np.var(portfolio_daily_returns)\n    if portfolio_volatility > max_volatility:\n        print(\"Exceeded max volatility:\", portfolio_volatility)\n        return -np.inf\n    else:\n        return portfolio_returns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32030b8ca3a2d4e6a6bf6744fdfd820641190f85"
      },
      "cell_type": "code",
      "source": "%%time\nfeatures, pfs = calculate_features(funds_df)\nprint(features.shape, len(pfs), pfs[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "399e808ab768547304a097d1e3d267a101ef3889"
      },
      "cell_type": "code",
      "source": "portfolio_performance(np.array([0,0,0,0,0,0,1]), funds_df.values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "677e3bccef2100013375efdce67a2aa8057fa7b2"
      },
      "cell_type": "code",
      "source": "funds_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94960119682a640b0e3af03ed4cfe6c8ec51bc72"
      },
      "cell_type": "code",
      "source": "def compute_X_y(\n        funds_df: pd.DataFrame,\n        n_features: int,\n        n_portfolios: int=18,\n        min_test_idx: int=3*year_days, \n        test_len: int=year_days,\n        max_train_len: int=None,\n    ):\n    max_test_idx = len(funds_df)\n    n_samples = max_test_idx - min_test_idx - test_len\n    X = np.zeros((n_samples, n_features))\n    all_performances = np.zeros((n_samples, n_portfolios))\n    y = np.zeros(n_samples).astype(np.int)\n    sample_weights = np.zeros(n_samples)\n    for idx in tqdm(range(n_samples)):\n        test_starting_idx = min_test_idx + idx\n        if max_train_len is None:\n            present_data = funds_df.iloc[:test_starting_idx]\n        else:\n            present_data = funds_df.iloc[max(0, test_starting_idx-max_train_len):test_starting_idx]\n        test_data = funds_df.iloc[test_starting_idx:test_starting_idx+test_len]\n        X[idx], portfolios = calculate_features(present_data)\n        portfolio_performances = np.array([portfolio_performance(p.as_weights(), test_data.values) for p in portfolios])\n        all_performances[idx] = portfolio_performances\n        y[idx] = np.argmax(portfolio_performances)\n        sample_weights[idx] = 1 + np.max(portfolio_performances) - np.mean(portfolio_performances[portfolio_performances > -np.inf])\n    return X, y, all_performances, sample_weights",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9583991efb7c8f90eaa48e55fcf6e5a965c36bb4"
      },
      "cell_type": "markdown",
      "source": "## 1st run: entire timespan available"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50c74b59eeff10710243c3a3ec0bda19560791b7"
      },
      "cell_type": "code",
      "source": "min_test_idx = 3*year_days\n\nn_portfolios = 18\nn_funds = 7\nn_portfolio_features = 3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4bb764b5bb015518f45fa973cb5eab0f9e1d0cf4",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "X, y, all_performances, sample_weights = compute_X_y(funds_df, n_features=180, min_test_idx=min_test_idx)\nX.shape, y.shape, X.dtype, y.dtype",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c8d993cd42b02068121016904f22c216b60cb5c"
      },
      "cell_type": "code",
      "source": "assert(n_portfolios*(n_funds+n_portfolio_features) == X.shape[1])\n# base-models were fitted on a rolling-window dataset, so we don't have the targets for its beginning:\nfunds_df.shape, X.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad73093bae9b944e7a8476f04110263b5414d816"
      },
      "cell_type": "code",
      "source": "# some utilities for converting numpy arrays back to indexed dataframes for simpler analysis:\nimport re\n\ndef generator_to_name(gen: str) -> str:\n    name = re.sub('funds_df', 'daily', gen)\n    name = re.sub('_table', '', name)\n    name = re.sub('\\w+_days', '', name)\n    name = re.sub('\\)+', '', name)\n    name = re.sub('[\\(, ]+', '_', name)\n    if name[-1] == '_':\n        return name[:-1]\n    else:\n        return name\n\nportfolio_names = list(map(generator_to_name, [\n    'ef_max_sharpe(funds_df)',\n    'ef_max_sharpe(monthly_table, month_days)',\n    'ef_max_sharpe(weekly_table, week_days)',\n    'ef_min_volatility(funds_df)',\n    'ef_min_volatility(monthly_table, month_days)',\n    'ef_min_volatility(weekly_table, week_days)',\n    'ef_efficient_risk(funds_df, 0.10)',\n    'ef_efficient_risk(funds_df, 0.05)',\n    'ef_efficient_risk(monthly_table, 0.10, month_days)',\n    'ef_efficient_risk(monthly_table, 0.05, month_days)',\n    'ef_efficient_risk(weekly_table, 0.10, week_days)',\n    'ef_efficient_risk(weekly_table, 0.05, week_days)',\n    'ef_efficient_return(funds_df, 0.04)',\n    'ef_efficient_return(funds_df, 0.08)',\n    'ef_efficient_return(monthly_table, 0.04, month_days)',\n    'ef_efficient_return(monthly_table, 0.08, month_days)',\n    'ef_efficient_return(weekly_table, 0.04, week_days)',\n    'ef_efficient_return(weekly_table, 0.08, week_days)',\n]))\n\nallocation_colnames = [\n    f'{basemodel}_{fundname}' \n    for basemodel in portfolio_names\n    for fundname in funds_df.columns\n]\nprint(len(allocation_colnames))\nperformance_colnames = [\n    f'{basemodel}_{metric}' \n    for basemodel in portfolio_names\n    for metric in ['e_returns', 'e_volatility', 'sharpe']\n]\nprint(len(performance_colnames))\nassert(len(performance_colnames)+len(allocation_colnames) == X.shape[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce41bd28cba684cee9ab35d1c17ee2ffffd8388e"
      },
      "cell_type": "code",
      "source": "portfolio_allocations = pd.DataFrame(\n    X[:,:n_portfolios*n_funds], \n    index=funds_df.index[min_test_idx:min_test_idx+X.shape[0]],\n    columns=allocation_colnames\n)\nportfolio_features = pd.DataFrame(\n    X[:,n_portfolios*n_funds:], \n    index=funds_df.index[min_test_idx:min_test_idx+X.shape[0]],\n    columns=performance_colnames\n)\nportfolio_test_performances = pd.DataFrame(\n    all_performances,\n    index=funds_df.index[min_test_idx:min_test_idx+all_performances.shape[0]],\n    columns=portfolio_names\n)\noptimal_portfolios = pd.Series(\n    y, \n    index=funds_df.index[min_test_idx:min_test_idx+y.shape[0]]\n).map(lambda i: portfolio_names[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da556b41e248444446f7e8fdced6902b1dbf5296"
      },
      "cell_type": "code",
      "source": "filename = \"18-funds-data-\" + str(pd.datetime.now().date())\nprint(filename)\nportfolio_allocations.to_csv(f\"{filename}-allocations.csv\", index=True)\nportfolio_features.to_csv(f\"{filename}-features.csv\", index=True)\nportfolio_test_performances.to_csv(f\"{filename}-test-performances.csv\", index=True)\noptimal_portfolios.to_csv(f\"{filename}-optimal-portfolios.csv\", index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "71910dd5ff29665393c5ffa9388f91107ae6924a"
      },
      "cell_type": "markdown",
      "source": "## 2nd run: model input contains no more than the last 5 years of data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef07ea45c8403214011e35c62a91e883fba30cab"
      },
      "cell_type": "code",
      "source": "min_test_idx = 3*year_days\nmax_train_len = 5*year_days",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4bb764b5bb015518f45fa973cb5eab0f9e1d0cf4",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "X, y, all_performances, sample_weights = compute_X_y(\n    funds_df, \n    n_features=180, \n    min_test_idx=min_test_idx,\n    max_train_len=max_train_len\n)\nX.shape, y.shape, X.dtype, y.dtype",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed9c7c6a17a434e431b28d68fc005f3ca8a4a608"
      },
      "cell_type": "code",
      "source": "portfolio_allocations = pd.DataFrame(\n    X[:,:n_portfolios*n_funds], \n    index=funds_df.index[min_test_idx:min_test_idx+X.shape[0]],\n    columns=allocation_colnames\n)\nportfolio_features = pd.DataFrame(\n    X[:,n_portfolios*n_funds:], \n    index=funds_df.index[min_test_idx:min_test_idx+X.shape[0]],\n    columns=performance_colnames\n)\nportfolio_test_performances = pd.DataFrame(\n    all_performances,\n    index=funds_df.index[min_test_idx:min_test_idx+all_performances.shape[0]],\n    columns=portfolio_names\n)\noptimal_portfolios = pd.Series(\n    y, \n    index=funds_df.index[min_test_idx:min_test_idx+y.shape[0]]\n).map(lambda i: portfolio_names[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3138e350a5b00c50bd9cded7a943a95288ba6a21"
      },
      "cell_type": "code",
      "source": "filename = f\"18-funds-data-clipped-{max_train_len}-{pd.datetime.now().date()}\"\nprint(filename)\nportfolio_allocations.to_csv(f\"{filename}-allocations.csv\", index=True)\nportfolio_features.to_csv(f\"{filename}-features.csv\", index=True)\nportfolio_test_performances.to_csv(f\"{filename}-test-performances.csv\", index=True)\noptimal_portfolios.to_csv(f\"{filename}-optimal-portfolios.csv\", index=True)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}